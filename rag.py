import os
import openai
from pinecone import Pinecone
from dotenv import load_dotenv
import json
import random

load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")

# Initialize Pinecone (new way)
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index_name = os.getenv("PINECONE_INDEX")
index = pc.Index(index_name)

# AI-only helper
def ai_only(prompt: str, max_tokens=600):
    try:
        chat_resp = openai.chat.completions.create(
            model="gpt-5-chat-latest",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=max_tokens
        )
        return chat_resp.choices[0].message.content.strip()
    except Exception as e:
        return f"Error in ai_only: {str(e)}"

def ask_bot(user_input, top_k=3):
    """
    General RAG chatbot for Burmese + English answers.
    Always uses default Pinecone namespace.
    """
    return _query_with_context(user_input=user_input, feature="general", top_k=top_k)


def content_checker(content, poster="", date="", platform="", top_k=3):
    """
    Content Checker chatbot.
    Adds metadata (poster, date, platform) to the system prompt.
    Uses default Pinecone namespace.
    """
    # Include metadata in user input to influence GPT reasoning
    user_input_with_meta = f"Poster: {poster}\nDate: {date}\nPlatform: {platform}\n\nContent:\n{content}"
    
    return _query_with_context(user_input=user_input_with_meta, feature="content_checker", top_k=top_k)


def _query_with_context(user_input, feature="general", top_k=3):
    """
    Internal helper to query Pinecone and build GPT prompt.
    Uses default namespace for all features.
    """
    # 1ï¸âƒ£ Embed user input
    try:
        emb_resp = openai.embeddings.create(
            model="text-embedding-3-small",
            input=user_input
        )
        query_vector = emb_resp.data[0].embedding
    except Exception as e:
        return f"Error creating embedding: {str(e)}"

    # 2ï¸âƒ£ Query Pinecone
    try:
        result = index.query(
            vector=query_vector,
            top_k=top_k,
            include_metadata=True,
        )
        context_texts = [match.metadata["text"] for match in result.matches]
    except Exception as e:
        context_texts = []
        print(f"Warning: Pinecone query failed: {str(e)}")

    # 3ï¸âƒ£ Build system prompt
    if feature == "general":
        system_prompt = (
            "You are LannPya Bot, a friendly cybersecurity guide for non-tech-savvy users in Myanmar. "
            "Always respond in Burmese using short and clear sentences. Explain in simple terms.\n"
            "Always use consistent pronouns: Refer to yourself as á€€á€»á€½á€”á€ºá€á€±á€¬á€º and the user as á€á€„á€º\n"
            "Use consistent pronouns in Burmese: á€€á€»á€½á€”á€ºá€á€±á€¬á€º for I (subject). á€€á€»á€½á€”á€ºá€á€±á€¬á€·á€º for me (object). á€€á€»á€½á€”á€ºá€á€±á€¬á€·á€ºá€›á€²á€· for my/mine (possessive). á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€º for myself. á€á€„á€º for you (subject). á€á€„á€·á€º for you (object). á€á€„á€·á€ºá€›á€²á€·  for your/yours (possessive). á€á€„á€ºá€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€º for yourself.\n"
            "Appreciate the user if he knows or can share specific data to help solve the problem.\n"
            "Always use an encouraging and supportive tone.\n"
            "Remind the user that they are doing well and that learning step by step is normal.\n"
            "Use polite particles like 'á€•á€«', 'á€™á€šá€º', 'á€á€šá€º' to keep a respectful and friendly tone.\n"
            "Sometimes use 'á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€·' (we) to make the user feel included, e.g., 'á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€á€­á€¯á€· á€¡á€á€°á€á€° á€œá€±á€·á€œá€¬á€€á€¼á€™á€šá€ºá‹'.\n"
            "Handle unclear inputs gently: If the user types something confusing, suggest politely what they might mean instead of rejecting directly.\n"
            "Use simple encouragements like 'á€¡á€†á€„á€ºá€•á€¼á€±á€™á€šá€º', 'á€™á€•á€°á€•á€«á€”á€²á€·', 'á€€á€±á€¬á€„á€ºá€¸á€•á€«á€•á€¼á€®', 'á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€™á€šá€º' to motivate the user.\n"
            "Occasionally use helpful emojis (âœ…, ğŸ”, ğŸ“±, ğŸ‘) to make responses friendlier, but never overuse them.\n"
            "Separate lines for better understanding and clear format.\n"
            "Bold the important keywords.\n"
            "Use Burmese words as much as possible but use English words for tech terms.\n"
            "All of your response must be either in English or in Burmses. Do not put any other language symbols.\n"
            "If user shares their name, start every response by calling their name.\n"
            "Conversation rules:\n"
            "1. If the user only says short acknowledgments like 'yes', 'ok', 'thanks', 'thank you', "
            "or emoji responses, respond politely but do not assume they are asking a new question.\n"
            "2. Only provide detailed guidance if the user asks a clear question about cybersecurity or privacy.\n"
            "3. If the user asks questions outside your scope (e.g., 'What's life?', general knowledge, or unrelated topics), "
            "politely refuse and remind them that you only provide guidance on cybersecurity and privacy.\n"
            "4. Keep answers short and to the point when the message is clearly an acknowledgment.\n"
            "Knowledge about laws:\n"
            "1. You have access to Myanmar Cybersecurity Law and related regulations uploaded in your knowledgebase.\n"
            "2. Always explain legal information accurately in Burmese using simple sentences.\n"
            "3. Highlight important **law numbers**, **chapters**, and **key points** in bold.\n"
            "4. If the user asks for a law reference, provide the **law number** and **chapter** clearly.\n"
            "5. Avoid giving legal advice beyond what the law text says; always stick to factual legal content.\n\n"
            "Knowledge base context:\n"
            + ("\n".join(context_texts) if context_texts else "[No relevant knowledgebase context]") +
            "\n\nUse this knowledgebase to analyze the content. "
            "If the answer is not found well enough in the knowledgebase, use your own knowledge."
        )
    else:  # content_checker
        system_prompt = (
            "You are LannPya Botâ€™s Content Checker. "
            "Analyze whether the given content is trustworthy or suspicious. "
            "Respond in Burmese with short, clear sentences. "
            "Greeting should always start with 'á€™á€„á€ºá€¹á€‚á€œá€¬á€•á€«' and polite tone in Burmese. "
            "Separate lines for better understanding and clear format. "
            "Your tone must sound as an educator or guide. "
            "Bold the important keywords. "
            "Use Burmese words as much but use English words for tech terms. "
            "If user shares his name, start every response by calling his name."
            "Respond only with analysis; do NOT ask follow-up questions or continue the conversation."
            "Do NOT ask any follow-up questions, suggest extra steps, or engage in conversation.\n"
            "Knowledge about laws:\n"
            "- You have access to Myanmar Cybersecurity Law and related regulations uploaded in your knowledgebase.\n"
            "- If the content is related to legal matters, mention the relevant **law numbers**, **chapters**, or **key points**.\n"
            "- Always explain legal information accurately in Burmese using simple, clear sentences.\n"
            "- Avoid giving legal advice; stick to factual information from the law texts.\n\n"
            "Knowledge base context:\n"
            + ("\n".join(context_texts) if context_texts else "[No relevant knowledgebase context]") +
            "Use this knowledgebase to analyze the content. "
            "If the answer is not found well enough in the knowledgebase, use your own knowledge."
        )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    # 4ï¸âƒ£ GPT completion
    try:
        chat_resp = openai.chat.completions.create(
            model="gpt-5-chat-latest",
            messages=messages,
            temperature=0.7,
            max_tokens=800
        )
        return chat_resp.choices[0].message.content.strip()
    except Exception as e:
        return f"Error in GPT response: {str(e)}"

    
def ask_bot_content_checker(content, poster, date, platform, top_k=3):
    """
    Content Checker using RAG + GPT.
    1ï¸âƒ£ Query Pinecone for top_k most relevant context.
    2ï¸âƒ£ Include metadata (poster/platform/date) in reasoning.
    3ï¸âƒ£ GPT uses knowledgebase first, falls back to its own knowledge if needed.
    """
    # 1ï¸âƒ£ Embed user content
    try:
        emb_resp = openai.embeddings.create(
            model="text-embedding-3-small",
            input=content
        )
        query_vector = emb_resp.data[0].embedding
    except Exception as e:
        return f"Error creating embedding: {str(e)}"

    # 2ï¸âƒ£ Query Pinecone (default namespace)
    try:
        result = index.query(
            vector=query_vector,
            top_k=top_k,
            include_metadata=True,
        )
        context_texts = [match.metadata["text"] for match in result.matches]
    except Exception as e:
        context_texts = []
        print(f"Warning: Pinecone query failed: {str(e)}")

# 3ï¸âƒ£ Build system prompt for Content Checker (English)
    system_prompt = (
        "You are LannPya Botâ€™s Content Checker. "
        "Analyze whether the given content is trustworthy or suspicious. "
        "Respond in Burmese with short, clear sentences. "
        "Greeting should always start with 'á€™á€„á€ºá€¹á€‚á€œá€¬á€•á€«' and polite tone in Burmese. "
        "Separate lines for better understanding and clear format. "
        "Your tone must sound as an educator or guide. "
        "Bold the important keywords. "
        "Use Burmese words as much but use English words for tech terms. "
        "If user shares his name, start every response by calling his name."
        "- Respond only with analysis; do NOT ask follow-up questions or continue the conversation."
        "- Do NOT ask any follow-up questions, suggest extra steps, or engage in conversation.\n"
        "Knowledge about laws:\n"
        "- You have access to Myanmar Cybersecurity Law and related regulations uploaded in your knowledgebase.\n"
        "- If the content is related to legal matters, mention the relevant **law numbers**, **chapters**, or **key points**.\n"
        "- Always explain legal information accurately in Burmese using simple, clear sentences.\n"
        "- Avoid giving legal advice; stick to factual information from the law texts.\n\n"
        "Knowledge base context:\n"
        + (
            "\n".join(context_texts)
            if context_texts
            else "[No relevant knowledgebase context]"
        )
        + 
        "Use this knowledgebase to analyze the content. "
        "If the answer is not found well enough in the knowledgebase, use your own knowledge."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": content}
    ]

    # 4ï¸âƒ£ GPT completion
    try:
        chat_resp = openai.chat.completions.create(
            model="gpt-5-chat-latest",
            messages=messages,
            temperature=0.6,
            max_tokens=800
        )
        return chat_resp.choices[0].message.content.strip()
    except Exception as e:
        return f"Error in GPT response: {str(e)}"
    
PREDEFINED_QUESTIONS = {
    "phishing": [
        "á€á€„á€·á€ºá€†á€®á€€á€­á€¯ á€™á€á€­á€á€±á€¬á€œá€°á€™á€¾ á€¡á€®á€¸á€™á€±á€¸á€œá€º (á€á€­á€¯á€·) á€™á€€á€ºá€†á€±á€·á€á€»á€º á€›á€›á€¾á€­á€á€²á€·á€á€œá€¬á€¸?",
        "á€‘á€­á€¯á€™á€€á€ºá€†á€±á€·á€á€»á€ºá€á€½á€„á€º á€œá€„á€·á€ºá€á€ºá€á€…á€ºá€á€¯á€€á€­á€¯ á€”á€¾á€­á€•á€ºá€›á€”á€º (á€á€­á€¯á€·) á€á€…á€ºá€á€¯á€á€¯ á€’á€±á€«á€„á€ºá€¸á€œá€¯á€’á€ºá€œá€¯á€•á€ºá€›á€”á€º á€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€‘á€¬á€¸á€á€œá€¬á€¸?",
        "áá€„á€ºá€¸á€á€½á€„á€º á€á€„á€·á€ºá€…á€€á€¬á€¸á€á€¾á€€á€ºáŠ á€˜á€á€ºá€¡á€á€»á€€á€ºá€¡á€œá€€á€º (á€á€­á€¯á€·) á€€á€­á€¯á€šá€ºá€›á€±á€¸á€€á€­á€¯á€šá€ºá€á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€á€½á€±á€€á€­á€¯ á€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€‘á€¬á€¸á€á€œá€¬á€¸?",
        "á€•á€­á€¯á€·á€á€°á á€œá€­á€•á€ºá€…á€¬ (á€á€­á€¯á€·) á€œá€„á€·á€ºá€á€ºá€á€Šá€º á€á€¶á€á€šá€–á€¼á€…á€ºá€–á€½á€šá€º (á€á€­á€¯á€·) á€‘á€°á€¸á€á€¼á€¬á€¸á€”á€±á€á€œá€¬á€¸?",
        "á€á€„á€º á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€á€…á€ºá€á€¯á€á€¯ á€™á€»á€¾á€á€±á€™á€­á€•á€¼á€®á€¸á€œá€¬á€¸ (á€á€­á€¯á€·) á€œá€„á€·á€ºá€á€ºá€€á€­á€¯ á€”á€¾á€­á€•á€ºá€™á€­á€•á€¼á€®á€¸á€œá€¬á€¸?"
    ],
    "scam": [
        "á€á€…á€ºá€…á€¯á€¶á€á€…á€ºá€šá€±á€¬á€€á€ºá€€ á€„á€½á€±á€€á€¼á€±á€¸ (á€á€­á€¯á€·) á€†á€¯á€œá€¬á€˜á€ºá€¡á€€á€¼á€±á€¬á€„á€ºá€¸ á€™á€»á€¾á€±á€¬á€ºá€œá€„á€·á€ºá€™á€‘á€¬á€¸á€˜á€² á€†á€€á€ºá€á€½á€šá€ºá€œá€¬á€á€œá€¬á€¸?",
        "á€á€„á€·á€ºá€€á€­á€¯ á€™á€¼á€”á€ºá€™á€¼á€”á€ºá€†á€”á€ºá€†á€”á€º á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€›á€”á€º á€–á€­á€¡á€¬á€¸á€•á€±á€¸á€á€²á€·á€á€œá€¬á€¸?",
        "á€‘á€°á€¸á€á€¼á€¬á€¸á€á€±á€¬á€”á€Šá€ºá€¸á€œá€™á€ºá€¸á€™á€»á€¬á€¸á€–á€¼á€„á€·á€º (á€†á€¯á€€á€’á€ºáŠ á€€á€›á€…á€ºá€•á€á€­á€¯á€„á€½á€±á€€á€¼á€±á€¸) á€„á€½á€±á€•á€±á€¸á€á€»á€±á€›á€”á€º á€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€á€²á€·á€á€œá€¬á€¸?",
        "á€‘á€­á€¯á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€€á€¼á€¬á€¸á€á€»á€€á€º (á€á€­á€¯á€·) á€á€±á€«á€ºá€†á€­á€¯á€á€°á€á€Šá€º á€á€›á€¬á€¸á€á€„á€ºá€™á€¾á€¯á€›á€¾á€­á€™á€›á€¾á€­ á€á€„á€º á€…á€…á€ºá€†á€±á€¸á€á€²á€·á€á€œá€¬á€¸?",
        "á€á€„á€º á€„á€½á€±á€•á€±á€¸á€á€»á€±á€•á€¼á€®á€¸á€á€½á€¬á€¸á€•á€¼á€®á€œá€¬á€¸ (á€á€­á€¯á€·) á€€á€­á€¯á€šá€ºá€›á€±á€¸á€€á€­á€¯á€šá€ºá€á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸ á€™á€»á€¾á€á€±á€™á€­á€•á€¼á€®á€¸á€œá€¬á€¸?"
    ],
    "hack": [
        "á€á€„á€·á€ºá€¡á€€á€±á€¬á€„á€·á€ºá€™á€»á€¬á€¸á€á€½á€„á€º á€•á€¯á€¶á€™á€¾á€”á€ºá€™á€Ÿá€¯á€á€ºá€á€±á€¬ á€œá€¾á€¯á€•á€ºá€›á€¾á€¬á€¸á€™á€¾á€¯á€™á€»á€¬á€¸ á€›á€¾á€­á€™á€›á€¾á€­ á€á€á€­á€‘á€¬á€¸á€™á€­á€á€œá€¬á€¸?",
        "á€™á€€á€¼á€¬á€á€±á€¸á€™á€®á€€ á€á€„á€·á€ºá€…á€€á€¬á€¸á€á€¾á€€á€ºá€€á€­á€¯ á€•á€¼á€”á€ºá€á€¯á€¶á€¸á€™á€­á€á€œá€¬á€¸ (á€á€­á€¯á€·) á€™á€»á€¾á€á€±á€™á€­á€á€œá€¬á€¸?",
        "á€á€„á€·á€ºá€•á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€á€±á€¬ á€¡á€€á€±á€¬á€„á€·á€ºá€á€…á€ºá€á€¯á€á€¯á€á€­á€¯á€· á€á€„á€ºá€›á€±á€¬á€€á€ºá á€™á€›á€á€±á€¬á€·á€á€œá€¬á€¸?",
        "á€‘á€°á€¸á€†á€”á€ºá€¸á€á€±á€¬ á€á€Šá€ºá€”á€±á€›á€¬á€™á€»á€¬á€¸á€™á€¾ á€¡á€€á€±á€¬á€„á€·á€ºá€á€„á€ºá€›á€±á€¬á€€á€ºá€™á€¾á€¯á€¡á€€á€¼á€±á€¬á€„á€ºá€¸ á€á€á€­á€•á€±á€¸á€á€»á€€á€ºá€™á€»á€¬á€¸ á€›á€›á€¾á€­á€á€²á€·á€á€œá€¬á€¸?",
        "á€á€„á€·á€ºá€‘á€¶á€á€½á€„á€·á€ºá€™á€á€±á€¬á€„á€ºá€¸á€˜á€² á€–á€­á€¯á€„á€ºá€™á€»á€¬á€¸ (á€á€­á€¯á€·) á€†á€€á€ºá€á€„á€ºá€™á€»á€¬á€¸ á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€á€½á€¬á€¸á€á€œá€¬á€¸?"
    ],
    "fraud": [
        "á€á€„á€·á€ºá€„á€½á€±á€€á€¼á€±á€¸ (á€á€­á€¯á€·) á€€á€­á€¯á€šá€ºá€›á€±á€¸á€€á€­á€¯á€šá€ºá€á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€á€½á€„á€·á€ºá€•á€¼á€¯á€á€»á€€á€ºá€™á€›á€¾á€­á€˜á€² á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¶á€›á€á€œá€¬á€¸?",
        "á€á€„á€·á€ºá€¡á€€á€±á€¬á€„á€·á€ºá€‘á€²á€á€½á€„á€º á€™á€á€­á€á€±á€¬ á€„á€½á€±á€œá€½á€¾á€²á€™á€¾á€¯á€™á€»á€¬á€¸ á€›á€¾á€­á€”á€±á€á€œá€¬á€¸?",
        "á€á€…á€ºá€…á€¯á€¶á€á€…á€ºá€šá€±á€¬á€€á€ºá€€ á€¡á€½á€”á€ºá€œá€­á€¯á€„á€ºá€¸ (á€á€­á€¯á€·) á€¡á€±á€¬á€·á€–á€ºá€œá€­á€¯á€„á€ºá€¸á€á€½á€„á€º á€á€„á€·á€ºá€€á€­á€¯á€šá€ºá€…á€¬á€¸ á€Ÿá€”á€ºá€†á€±á€¬á€„á€ºá€”á€±á€á€œá€¬á€¸?",
        "á€á€¶á€á€šá€–á€¼á€…á€ºá€–á€½á€šá€º á€œá€¯á€•á€ºá€„á€”á€ºá€¸ (á€á€­á€¯á€·) á€á€€á€ºá€˜á€ºá€†á€­á€¯á€€á€ºá€á€…á€ºá€á€¯á€‘á€¶ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸ á€•á€±á€¸á€á€²á€·á€™á€­á€á€œá€¬á€¸?",
        "á€‘á€­á€¯á€€á€²á€·á€á€­á€¯á€·á€á€±á€¬ á€•á€¼á€¿á€”á€¬ á€šá€á€„á€ºá€€ á€–á€¼á€…á€ºá€–á€°á€¸á€•á€«á€á€œá€¬á€¸?"
    ],
    "threat": [
        "á€á€„á€·á€ºá€€á€­á€¯ á€¡á€½á€”á€ºá€œá€­á€¯á€„á€ºá€¸ (á€á€­á€¯á€·) á€¡á€±á€¬á€·á€–á€ºá€œá€­á€¯á€„á€ºá€¸á€™á€¾ á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€ºá€á€¼á€­á€™á€ºá€¸á€á€¼á€±á€¬á€€á€ºá€™á€¾á€¯ á€›á€›á€¾á€­á€á€²á€·á€á€œá€¬á€¸?",
        "á€‘á€­á€¯á€™á€€á€ºá€†á€±á€·á€á€»á€ºá€á€½á€„á€º á€á€„á€·á€ºá€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€ºáŠ á€™á€­á€á€¬á€¸á€…á€¯ (á€á€­á€¯á€·) á€•á€­á€¯á€„á€ºá€†á€­á€¯á€„á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€”á€¹á€á€›á€¬á€šá€ºá€•á€¼á€¯á€™á€Šá€ºá€Ÿá€¯ á€–á€±á€¬á€ºá€•á€¼á€‘á€¬á€¸á€á€œá€¬á€¸?",
        "á€„á€½á€±á€€á€¼á€±á€¸ (á€á€­á€¯á€·) á€á€¯á€¶á€·á€•á€¼á€”á€ºá€™á€¾á€¯á€á€…á€ºá€…á€¯á€¶á€á€…á€ºá€›á€¬á€¡á€á€½á€€á€º áá€„á€ºá€¸á€á€­á€¯á€·á€€ á€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€‘á€¬á€¸á€á€œá€¬á€¸?",
        "á€‘á€­á€¯á€á€¼á€­á€™á€ºá€¸á€á€¼á€±á€¬á€€á€ºá€™á€¾á€¯á€á€Šá€º á€‘á€•á€ºá€€á€¬á€‘á€•á€ºá€€á€¬ á€›á€›á€¾á€­á€á€œá€¬á€¸ (á€á€­á€¯á€·) á€”á€±á€›á€¬á€¡á€™á€»á€­á€¯á€¸á€™á€»á€­á€¯á€¸á€™á€¾ á€›á€›á€¾á€­á€á€œá€¬á€¸?",
        "á€‘á€­á€¯á€á€¼á€­á€™á€ºá€¸á€á€¼á€±á€¬á€€á€ºá€™á€¾á€¯á€€á€­á€¯ á€¡á€¬á€á€¬á€•á€­á€¯á€„á€ºá€á€…á€ºá€á€¯á€á€¯á€á€­á€¯á€· á€á€á€„á€ºá€¸á€•á€­á€¯á€·á€á€„á€ºá€•á€¼á€á€²á€·á€•á€«á€á€œá€¬á€¸?"
    ],
    "malware": [
        "á€á€„á€·á€ºá€…á€€á€ºá€•á€…á€¹á€…á€Šá€ºá€¸ á€•á€¯á€¶á€™á€¾á€”á€ºá€‘á€€á€º á€”á€¾á€±á€¸á€€á€½á€±á€¸á€”á€±á€á€œá€¬á€¸ (á€á€­á€¯á€·) á€•á€¯á€¶á€™á€¾á€”á€ºá€™á€Ÿá€¯á€á€ºá€˜á€² á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€”á€±á€á€œá€¬á€¸?",
        "á€¡á€á€Šá€ºá€™á€•á€¼á€¯á€›á€á€±á€¸á€á€±á€¬ á€†á€±á€¬á€·á€–á€ºá€á€²á€á€…á€ºá€á€¯á€á€¯á€€á€­á€¯ á€’á€±á€«á€„á€ºá€¸á€œá€¯á€’á€ºá€œá€¯á€•á€º (á€á€­á€¯á€·) á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€á€²á€·á€á€œá€¬á€¸?",
        "á€á€„á€º á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€á€²á€·á€á€¼á€„á€ºá€¸á€™á€›á€¾á€­á€á€±á€¬ á€•á€±á€«á€·á€¡á€•á€ºá€™á€»á€¬á€¸ (á€á€­á€¯á€·) á€•á€›á€­á€¯á€‚á€›á€™á€ºá€™á€»á€¬á€¸ á€™á€¼á€„á€ºá€”á€±á€›á€á€œá€¬á€¸?",
        "á€—á€­á€¯á€„á€ºá€¸á€›á€•á€ºá€…á€ºá€€á€¬á€€á€½á€šá€ºá€›á€±á€¸á€†á€±á€¬á€·á€–á€ºá€á€²á€™á€¾ á€á€…á€ºá€á€¯á€á€¯á€€á€­á€¯ á€™á€€á€¼á€¬á€á€±á€¸á€™á€®á€€ á€…á€…á€ºá€†á€±á€¸á€á€½á€±á€·á€›á€¾á€­ (á€á€­á€¯á€·) á€•á€­á€á€ºá€†á€­á€¯á€·á€‘á€¬á€¸á€á€œá€¬á€¸?",
        "á€–á€­á€¯á€„á€ºá€™á€»á€¬á€¸ á€•á€»á€±á€¬á€€á€ºá€†á€¯á€¶á€¸ (á€á€­á€¯á€·) á€…á€¬á€á€¾á€€á€ºá€á€½á€¬á€¸á€á€¶á€›á€á€œá€¬á€¸?"
    ]
}

def generate_followup_questions(topic, user_answers):
    """
    Given first 5 answers, generate 5 more questions dynamically using OpenAI.
    """
    combined_text = "\n".join([f"Q: {q} â†’ A: {a}" for q, a in user_answers.items()])
    prompt = f"""
    Based on the user's answers to the following core cybersecurity questions:
    {combined_text}

    Generate 5 follow-up diagnostic questions for the same topic in Burmese.
    Output as a numbered list, short questions only.
    """
    followup_qs = ai_only(prompt).split("\n")
    questions = []
    for q in followup_qs:
        q = q.strip()
        if q:
            questions.append({"question": q, "type": "mcq_text"})  # mcq + optional textbox
    return questions[:5]  # only 5

# Scenario Simulation in rag.py
def get_scenario_questions(topic, stage=1, user_answers=None):
    """
    Returns questions for scenario simulation.
    - stage=1: first 5 core questions
    - stage=2: generate 5 follow-up questions based on first 5 answers
    - stage=others: fully AI-generated
    Each question is a dict: {"question": str, "type": "mcq_text"/"text"}
    """
    questions = []

    if topic in PREDEFINED_QUESTIONS:
        if stage == 1:
            for q in PREDEFINED_QUESTIONS[topic]:
                questions.append({"question": q, "type": "mcq_text"})
        elif stage == 2 and user_answers:
            questions = generate_followup_questions(topic, user_answers)
        # Always add free-text at the end
        questions.append({"question": "Any additional details?", "type": "text"})
    else:
        # Fully AI-generated 10 questions (text input)
        prompt = "Generate 10 short cybersecurity diagnostic questions in Burmese for a general unknown threat."
        text_qs = ai_only(prompt).split("\n")[:10]
        for q in text_qs:
            questions.append({"question": q, "type": "text"})
        questions.append({"question": "Any additional details?", "type": "text"})
    return questions


def analyze_scenario_responses(topic, user_answers):
    """
    Analyze user answers:
    1ï¸âƒ£ Pinecone retrieval
    2ï¸âƒ£ OpenAI analysis
    Returns risks + solutions
    """
    # Prepare combined text
    combined_text = "\n".join([f"Q: {q} â†’ A: {a}" for q, a in user_answers.items()])

    # Step 1: Pinecone + OpenAI
    pinecone_context = _query_with_context(combined_text, feature="general", top_k=5)

    # Step 2: Refined OpenAI analysis
    final_prompt = f"""
    You are LannPya Bot, a Scenario Simulation Assistant.

    User Topic: {topic}

    User's Responses:
    {combined_text}

    Pinecone Context:
    {pinecone_context}

    Instructions:
    - Analyze the above information and summarize the main **RISKS** and actionable **SOLUTIONS**.
    - If any risks or solutions are related to legal requirements, mention the relevant **Myanmar Cybersecurity Law numbers**, **chapters**, or **key points**.
    - Respond in **Burmese**, clearly and concisely.
    - Use line breaks to separate each risk and solution for clarity.
    - Bold the most important keywords.
    - Use Burmese words as much as possible, and English words for technical terms.
    - Respond only with analysis; do NOT ask follow-up questions or continue the conversation.
    - Do NOT ask any follow-up questions, suggest extra steps, or engage in conversation.
    - Polite, professional, and educator-like tone.
    - Use few emojis to emphasize meanings of keywords (not too many).

    Output Format Example:
    **RISKS (á€¡á€”á€¹á€á€›á€¬á€šá€º):**
    1. <risk description>
    2. <risk description>

    **SOLUTIONS (á€–á€¼á€±á€›á€¾á€„á€ºá€¸á€á€»á€€á€º):**
    1. <solution description>
    2. <solution description>

    **LEGAL REFERENCES (á€¥á€•á€’á€±á€Šá€½á€¾á€”á€ºá€€á€¼á€¬á€¸á€á€»á€€á€º) [if relevant]:**
    - Mention relevant **law numbers** and **chapters** from the Myanmar Cybersecurity Law as needed.
    - Only include factual information from the law; do not give legal advice.
    """
    
    final_result = ai_only(final_prompt, max_tokens=1000)
    return final_result

def load_quiz_data(path="your_data.json"):
    """Load quiz data JSON safely."""
    with open(path, "r", encoding="utf-8") as f:
        data = f.read().strip()
        if not data:
            raise ValueError("JSON file is empty!")
        return json.loads(data)


def parse_quiz_text(ai_text):
    """
    Converts AI output into a structured list of questions.
    AI should return something like:
    1. Question?
       a) Option1
       b) Option2
       c) Option3
       d) Option4
       Answer: a
    """
    import re
    questions = []
    if not ai_text or not ai_text.strip():
        return questions

    try:
        text = ai_text.replace("\r\n", "\n").replace("\r", "\n").strip()

        # Primary split by blank lines or lines of dashes
        raw_blocks = [b for b in re.split(r"\n\s*(?:\n|[-=]{3,,}\n)+", text) if b.strip()]
        for block in raw_blocks:
            try:
                lines = [ln.strip() for ln in block.split("\n") if ln.strip()]
                if not lines:
                    continue

                q_line = re.sub(r"^\s*\d+[\.)]\s*", "", lines[0]).strip()
                question_text = q_line

                option_pattern = re.compile(r"^([a-dA-D])\s*[\)\.:\-]\s*(.+)$")
                label_to_option = {}
                answer_letter = None
                answer_text = None

                for ln in lines[1:]:
                    if re.match(r"(?i)^answer\s*[:\-]", ln):
                        m_ans_letter = re.search(r"(?i)answer\s*[:\-]\s*([a-dA-D])\b", ln)
                        if m_ans_letter:
                            answer_letter = m_ans_letter.group(1).lower()
                        else:
                            m_ans_text = re.search(r"(?i)answer\s*[:\-]\s*(.+)$", ln)
                            if m_ans_text:
                                answer_text = m_ans_text.group(1).strip()
                        continue

                    m = option_pattern.match(ln)
                    if m:
                        label = m.group(1).lower()
                        text_val = m.group(2).strip()
                        if label not in label_to_option:
                            label_to_option[label] = text_val

                if not all(k in label_to_option for k in ["a", "b", "c", "d"]):
                    continue

                if answer_letter and answer_letter in label_to_option:
                    correct_option_text = label_to_option[answer_letter]
                elif answer_text:
                    norm = answer_text.strip().lower()
                    correct_option_text = next((opt for opt in label_to_option.values() if opt.strip().lower() == norm), None)
                else:
                    correct_option_text = None

                if not correct_option_text:
                    for opt_txt in label_to_option.values():
                        if re.search(r"(?i)\b(correct|true|right)\b", opt_txt):
                            correct_option_text = opt_txt
                            break

                if not correct_option_text:
                    continue

                ordered_options = [label_to_option[k] for k in ["a", "b", "c", "d"]]
                questions.append({
                    "question": question_text,
                    "options": ordered_options,
                    "answer": correct_option_text
                })
            except Exception as inner_e:
                print("Warning: Skipping malformed quiz block:", inner_e)

    except Exception as e:
        print("Error parsing AI quiz text:", e)

    return questions[:10]

def generate_quiz_from_topic(topic_name, difficulty: str | None = None):
    """
    Generate quiz from JSON topic using OpenAI.
    """
    with open("knowledge_base.json", "r", encoding="utf-8") as f:
        quiz_data = json.load(f)
        
    if topic_name not in quiz_data:
        print(f"Topic '{topic_name}' not found in JSON.")
        return []

    topic_text = "\n".join(quiz_data[topic_name])  # all lines for that topic
    diff_text = (difficulty or "medium").lower()
    if diff_text not in {"easy", "medium", "hard"}:
        diff_text = "medium"

    base_prompt = f"""
    You are an expert quiz creator.
    Create 10 multiple-choice questions (1 correct + 3 wrong answers each) from the following text:

    {topic_text}

    Difficulty: {diff_text}

    Return questions in EXACTLY this format, with a blank line between questions, no extra commentary:
    1. Question text in Burmese?
       a) Option 1
       b) Option 2
       c) Option 3
       d) Option 4
       Answer: a
    """

    prompts = [
        base_prompt,
        base_prompt + "\nOnly output the items in the specified format. Do not add explanations, headings, or anything else.",
        base_prompt + "\nAbsolutely do not include any text outside the 10 questions in the specified format.",
    ]

    for attempt, prompt in enumerate(prompts, start=1):
        try:
            response = openai.chat.completions.create(
                model="gpt-5-chat-latest",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
                max_tokens=1500
            )
            ai_text = response.choices[0].message.content.strip()
            questions = parse_quiz_text(ai_text)
            if questions:
                return questions
            else:
                print(f"Warning: attempt {attempt} parsed 0 questions.")
        except Exception as e:
            print(f"Error generating quiz on attempt {attempt}:", e)

    # If still nothing, return empty list (caller may decide how to handle)
    return []

def generate_random_tip():
    """
    Returns a random cybersecurity tip using AI from the knowledge base.
    """
    # Load knowledge base once
    with open("knowledge_base.json", "r", encoding="utf-8") as f:
        kb = json.load(f)
    
    topic = random.choice(list(kb.keys()))
    content = "\n".join(kb[topic])

    prompt = (
        f"Create a short, practical cybersecurity tip from the following information:\n"
        f"{content}\n"
        f"Keep it under 10 words."
        "Use Burmese but you can use English technical terms where needed."
    )

    try:
        response = openai.chat.completions.create(
            model="gpt-5-chat-latest",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=1000
        )
        tip = response.choices[0].message.content.strip()
    except Exception as e:
        print("Error generating tip:", e)
        tip = "Stay safe online! (Could not generate AI tip.)"

    return {"tip": tip, "topic": topic}